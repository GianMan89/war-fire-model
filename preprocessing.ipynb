{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import rasterio as rio\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_unzip_files(folder_path, unzip_dbf=False):\n",
    "    # List all files in the folder\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), folder_path)\n",
    "            all_files.append(relative_path)\n",
    "    # If there are any dbf zip files, unzip them\n",
    "    if unzip_dbf:\n",
    "        for file_path in files:\n",
    "            if file_path.endswith('.zip'):\n",
    "                if not os.path.exists(f\"{folder_path}/{file_path[:-4]}\"):\n",
    "                    with zipfile.ZipFile(f\"{folder_path}/{file_path}\", 'r') as zip_ref:\n",
    "                        zip_ref.extractall(folder_path)\n",
    "    return all_files\n",
    "\n",
    "# Define the folder paths for shapefiles and csv files\n",
    "folder_path = 'input_data/raw/fires'\n",
    "files = list_unzip_files(folder_path, unzip_dbf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files, folder_path):\n",
    "    dataframes = []\n",
    "    # Load all shp and csv files\n",
    "    for file_path in files:\n",
    "        if file_path.endswith('.shp'):\n",
    "            df = gpd.read_file(f\"{folder_path}/{file_path}\")\n",
    "        elif file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(f\"{folder_path}/{file_path}\")\n",
    "        else:\n",
    "            continue\n",
    "        df.columns = map(str.upper, df.columns)\n",
    "        dataframes.append(df)\n",
    "    # Concatenate all dataframes\n",
    "    fire_data = pd.concat(dataframes, ignore_index=True)\n",
    "    return fire_data\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "fire_data = load_data(files, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fires(fire_data):\n",
    "    # Drop duplicates time and location\n",
    "    fire_data.drop_duplicates(subset=['ACQ_DATE', 'ACQ_TIME', 'LATITUDE', 'LONGITUDE'], inplace=True)\n",
    "    # Drop unnecessary columns\n",
    "    fire_data.drop(columns=['COUNTRY_ID', 'SCAN', 'TRACK', 'SATELLITE', 'CONFIDENCE', 'VERSION', 'FRP', 'DAYNIGHT', 'BRIGHTNESS', 'BRIGHT_T31', 'TYPE', 'GEOMETRY', 'INSTRUMENT'], inplace=True)\n",
    "    # Drop rows with missing or nan values\n",
    "    fire_data.dropna(inplace=True)\n",
    "    # Transform the date to datetime format\n",
    "    fire_data['ACQ_DATE'] = fire_data['ACQ_DATE'].apply(lambda x: pd.to_datetime(x).date())\n",
    "    # Transform the time to integer format\n",
    "    fire_data['ACQ_TIME'] = fire_data['ACQ_TIME'].astype(int)\n",
    "    # Reduce the precision of the coordinates using two decimal places, which is approximately 1.1 km\n",
    "    # (https://support.oxts.com/hc/en-us/articles/115002885125-Level-of-Resolution-of-Longitude-and-Latitude-Measurements)\n",
    "    fire_data['LATITUDE'] = fire_data['LATITUDE'].round(2)\n",
    "    fire_data['LONGITUDE'] = fire_data['LONGITUDE'].round(2)\n",
    "    return fire_data\n",
    "\n",
    "# Preprocess the fire data\n",
    "fire_data = preprocess_fires(fire_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fires_date(fire_data, start_date, end_date):\n",
    "    # Filter the fire data based on the input parameters\n",
    "    filtered_data = fire_data[(fire_data['ACQ_DATE'] >= start_date) & (fire_data['ACQ_DATE'] <= end_date)]\n",
    "    # Reset the index\n",
    "    filtered_data.reset_index(drop=True, inplace=True)\n",
    "    return filtered_data\n",
    "\n",
    "# Filter the fire data based on a specific date range\n",
    "start_date = pd.to_datetime('2014-12-01').date() # Need the last month of 2014 for calculating rolling statistics\n",
    "end_date = pd.to_datetime('today').date()\n",
    "fire_data = filter_fires_date(fire_data, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_border(data, path_to_border):\n",
    "    # Load the shapefile containing the administrative borders of Ukraine\n",
    "    ukraine_borders = gpd.read_file(path_to_border)\n",
    "    # Ensure the data is a GeoDataFrame\n",
    "    data_gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data.LONGITUDE, data.LATITUDE))\n",
    "    # Set the same coordinate reference system (CRS) for both GeoDataFrames\n",
    "    data_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    ukraine_borders.set_crs(epsg=4326, inplace=True)\n",
    "    # Perform a spatial join to filter datapoints within Ukraine borders\n",
    "    data_in_ukraine = gpd.sjoin(data_gdf, ukraine_borders, how='inner')\n",
    "    # Drop the geometry column as it's no longer needed\n",
    "    data_in_ukraine.drop(columns=['geometry', 'source', 'name', 'index_right'], inplace=True)\n",
    "    # Reset the index\n",
    "    data_in_ukraine.reset_index(drop=True, inplace=True)\n",
    "    # Make sure all columns are in uppercase\n",
    "    data_in_ukraine.columns = map(str.upper, data_in_ukraine.columns)\n",
    "    # Rename the ID column to OBLAST_ID\n",
    "    data_in_ukraine.rename(columns={'ID': 'OBLAST_ID'}, inplace=True)\n",
    "    # Generate a unique identifier for each grid cell\n",
    "    data_in_ukraine['GRID_CELL'] = data_in_ukraine['LATITUDE'].astype(str) + '_' + data_in_ukraine['LONGITUDE'].astype(str)\n",
    "    return data_in_ukraine\n",
    "\n",
    "# Filter the fire data based on the administrative borders of Ukraine\n",
    "fire_data = filter_data_border(fire_data, 'input_data/raw/ukr_borders/ua.shp') # https://simplemaps.com/gis/country/ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_counts(fire_data):\n",
    "    # sort by date\n",
    "    fire_data = fire_data.sort_values(by='ACQ_DATE')\n",
    "    # Generate the day of the year\n",
    "    fire_data['DAY_OF_YEAR'] = fire_data['ACQ_DATE'].apply(lambda x: x.timetuple().tm_yday)\n",
    "    # Generate the number of fires per grid cell for the specific day (ACQ_DATE)\n",
    "    fire_data['FIRE_COUNT_CELL'] = fire_data.groupby(['GRID_CELL', 'ACQ_DATE'])['ACQ_DATE'].transform('count')\n",
    "    # Generate the number of fires of the neighboring grid cells for the specific day (ACQ_DATE) by rounding to 0 and 1 decimal place\n",
    "    for i, k in zip([0, 1], [\"100km\", \"10km\"]):\n",
    "        fire_data[f'GRID_CELL_NEIGHBOR_{k}'] = (fire_data['LATITUDE']).round(i).astype(str) + '_' + (fire_data['LONGITUDE']).round(i).astype(str)\n",
    "        fire_data[f'FIRE_COUNT_CELL_NEIGHBOR_{k}'] = fire_data.groupby([f'GRID_CELL_NEIGHBOR_{k}', 'ACQ_DATE'])['ACQ_DATE'].transform('count')\n",
    "    # Generate the number of fires per region (ID) for the specific day (ACQ_DATE)\n",
    "    fire_data['FIRE_COUNT_OBLAST'] = fire_data.groupby(['OBLAST_ID', 'ACQ_DATE'])['ACQ_DATE'].transform('count')\n",
    "    # Generate average number of fires per grid cell, neighboring grid cells, and region for the last 7 and 30 days\n",
    "    for i in [7, 30]:\n",
    "        fire_data[f'FIRE_COUNT_CELL_AVG_{i}D'] = fire_data.groupby('GRID_CELL')['FIRE_COUNT_CELL'].transform(lambda x: x.rolling(i, min_periods=1).mean())\n",
    "        for j, k in zip([0, 1], [\"100km\", \"10km\"]):\n",
    "            fire_data[f'FIRE_COUNT_CELL_NEIGHBOR_{k}_AVG_{i}D'] = fire_data.groupby(f'GRID_CELL_NEIGHBOR_{k}')[f'FIRE_COUNT_CELL_NEIGHBOR_{k}'].transform(lambda x: x.rolling(i, min_periods=1).mean())\n",
    "        fire_data[f'FIRE_COUNT_OBLAST_AVG_{i}D'] = fire_data.groupby('OBLAST_ID')['FIRE_COUNT_OBLAST'].transform(lambda x: x.rolling(i, min_periods=1).mean())\n",
    "    return fire_data\n",
    "\n",
    "# Perform feature engineering\n",
    "fire_data = generate_counts(fire_data)\n",
    "# Filter the fire data based on a specific date range\n",
    "start_date = pd.to_datetime('2015-01-01').date()\n",
    "fire_data = filter_fires_date(fire_data, start_date, end_date)\n",
    "# Save the preprocessed fire data\n",
    "fire_data.to_csv('input_data/processed/fire_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_population_density(path_to_population, round_precision=2):\n",
    "    # Load the population density data\n",
    "    pop_density_df = pd.read_csv(path_to_population)\n",
    "    # Rename the columns to be more descriptive\n",
    "    pop_density_df.columns = ['LONGITUDE', 'LATITUDE', 'POP_DENSITY']\n",
    "    # Round the latitude and longitude to match the precision used in fire_data\n",
    "    pop_density_df['LATITUDE'] = pop_density_df['LATITUDE'].round(round_precision)\n",
    "    pop_density_df['LONGITUDE'] = pop_density_df['LONGITUDE'].round(round_precision)\n",
    "    # Calculate the average population density for each grid cell\n",
    "    pop_density_avg = pop_density_df.groupby(['LATITUDE', 'LONGITUDE'])['POP_DENSITY'].mean().reset_index()\n",
    "    # Create a unique identifier for each grid cell\n",
    "    pop_density_avg['GRID_CELL'] = pop_density_avg['LATITUDE'].astype(str) + '_' + pop_density_avg['LONGITUDE'].astype(str)\n",
    "    # Rename the column to indicate it's the population density average per grid cell\n",
    "    pop_density_avg.rename(columns={'POP_DENSITY': 'POP_DENSITY_CELL_AVG'}, inplace=True)\n",
    "    # Generate the average population density for the neighboring grid cells\n",
    "    for i, k in zip([0, 1], [\"100km\", \"10km\"]):\n",
    "        pop_density_avg[f'GRID_CELL_NEIGHBOR_{k}'] = (fire_data['LATITUDE']).round(i).astype(str) + '_' + (fire_data['LONGITUDE']).round(i).astype(str)\n",
    "        pop_density_avg[f'POP_DENSITY_CELL_NEIGHBOR_{k}_AVG'] = pop_density_avg.groupby(f'GRID_CELL_NEIGHBOR_{k}')['POP_DENSITY_CELL_AVG'].transform('mean')\n",
    "        pop_density_avg.drop(columns=[f'GRID_CELL_NEIGHBOR_{k}'], inplace=True)\n",
    "    return pop_density_avg\n",
    "\n",
    "# Calculate the population density for each grid cell\n",
    "pop_density = generate_population_density('input_data/raw/population/ukr_pd_2020_1km_UNadj_ASCII_XYZ.csv')\n",
    "# Save the population density data to a csv file\n",
    "pop_density.to_csv('input_data/processed/pop_density.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fire_pop_data(fire_data, pop_density):\n",
    "    # Drop redundant columns\n",
    "    pop_density.drop(columns=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
    "    # Merge the fire data with the population density data\n",
    "    fire_data = fire_data.merge(pop_density, on='GRID_CELL', how='left')\n",
    "    # Fill missing values with the average population density\n",
    "    fire_data['POP_DENSITY_CELL_AVG'].fillna(fire_data['POP_DENSITY_CELL_AVG'].mean(), inplace=True)\n",
    "    for k in [\"100km\", \"10km\"]:\n",
    "        fire_data[f'POP_DENSITY_CELL_NEIGHBOR_{k}_AVG'].fillna(fire_data[f'POP_DENSITY_CELL_NEIGHBOR_{k}_AVG'].mean(), inplace=True)\n",
    "    return fire_data\n",
    "\n",
    "# Load the population density data\n",
    "pop_density = pd.read_csv('input_data/processed/pop_density.csv')\n",
    "# Load the preprocessed fire data\n",
    "fire_data = pd.read_csv('input_data/processed/fire_data.csv')\n",
    "# Merge the fire data with the population density data\n",
    "fire_data = merge_fire_pop_data(fire_data, pop_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WGS84 projection (EPSG:4326)\n",
    "wgs84_proj = Proj('epsg:4326')\n",
    "\n",
    "def load_tif_with_reduced_resolution(tif_path, scale_factor):\n",
    "    with rio.open(tif_path) as src:\n",
    "        # Calculate the new shape\n",
    "        new_height = int(src.height * scale_factor)\n",
    "        new_width = int(src.width * scale_factor)\n",
    "        # Read the data with resampling\n",
    "        data = src.read(\n",
    "            out_shape=(src.count, new_height, new_width),\n",
    "            resampling=rio.enums.Resampling.bilinear\n",
    "        )\n",
    "        # Scale the transform\n",
    "        transform_scaled = src.transform * src.transform.scale(\n",
    "            (src.width / data.shape[-1]),\n",
    "            (src.height / data.shape[-2])\n",
    "        )\n",
    "        # Get the UTM projection from the metadata\n",
    "        utm_proj = Proj(src.crs)\n",
    "        return data, np.array(transform_scaled), utm_proj\n",
    "\n",
    "def transform_tif_coordinates(data, transform_tif, utm_proj):\n",
    "    # transform the coordinates of the pixels\n",
    "    lon = np.zeros(data.shape[1:])\n",
    "    lat = np.zeros(data.shape[1:])\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(data.shape[2]):\n",
    "            lat[i][j], lon[i][j] = transform(utm_proj, wgs84_proj, transform_tif[2] + transform_tif[0] * j, transform_tif[5] + transform_tif[4] * i)\n",
    "    return lon, lat\n",
    "\n",
    "def convert_to_dataframe(data, lon, lat, map_dict):\n",
    "    # Create a dataframe with the class of the pixel and its longitude and latitude value\n",
    "    df = pd.DataFrame(data[0].flatten(), columns=['class'])\n",
    "    # get longitude and latitude values and round them to 2 decimal places\n",
    "    df['LONGITUDE'] = lon.flatten().round(2)\n",
    "    df['LATITUDE'] = lat.flatten().round(2)\n",
    "    # Delete all rows with class 0 (no data)\n",
    "    df = df[df['class'] != 0]\n",
    "    # Map the classes to the new classes\n",
    "    df['CLASS'] = df['class'].map(map_dict)\n",
    "    df.drop(columns=['class'], inplace=True)\n",
    "    # Make sure all columns are in uppercase\n",
    "    df.columns = map(str.upper, df.columns)\n",
    "    return df\n",
    "\n",
    "# Load the TIFF file with reduced resolution\n",
    "# Reduce resolution by 90%, i.e., 10m resolution to 100m resolution\n",
    "scale_factor = 0.1 # TODO change to 0.1\n",
    "# Dictionary to merge some of the classes into new classes\n",
    "# Original classes: 0: 'No data', 1: 'Water', 2: 'Trees', 4: 'Flooded vegetation', 5: 'Crops', 7: 'Built Area', 8: 'Bare ground', 9: 'Snow and ice', 10: 'Clouds', 11: 'Rangeland'\n",
    "class_transform = {0: 0, 1: 0, 2: 1, 4: 2, 5: 3, 7: 4, 8: 2, 9: 0, 10: 0, 11: 2}\n",
    "\n",
    "# Iterate over all tif files in the folder\n",
    "for tif_file in list_unzip_files('input_data/raw/land_use'):\n",
    "    if tif_file.endswith('.tif'):\n",
    "        # Load the TIFF file with reduced resolution\n",
    "        data, transform_scaled, utm_proj = load_tif_with_reduced_resolution(f\"input_data/raw/land_use/{tif_file}\", scale_factor)\n",
    "        # Transform the coordinates of the pixels\n",
    "        lon, lat = transform_tif_coordinates(data, transform_scaled, utm_proj)\n",
    "        # Convert the data to a dataframe\n",
    "        land_use_df = convert_to_dataframe(data, lon, lat, class_transform)\n",
    "        # Save dataframe as a csv file\n",
    "        land_use_df.to_csv(f'input_data/processed/{tif_file[:-4]}_010.csv', index=False)\n",
    "        print(f\"Processed {tif_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(dataframes):\n",
    "    # Merge the dataframes\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    # Drop rows that are not in the Ukraine borders\n",
    "    merged_df = filter_data_border(merged_df, 'input_data/raw/ukr_borders/ua.shp')\n",
    "    # Reset the index\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def class_percentage_cell(land_use_data):\n",
    "    # One hot encode the class column in land_use_data\n",
    "    land_use_percentage = pd.get_dummies(land_use_data, columns=['CLASS'], prefix='LAND_USE_CLASS', drop_first=True)\n",
    "    # Drop the OBLAST_ID column as it's not needed\n",
    "    land_use_percentage.drop(columns=['OBLAST_ID'], inplace=True)\n",
    "    # Calculate the percentage of each class for each grid cell\n",
    "    land_use_percentage = land_use_percentage.groupby(['GRID_CELL']).mean()\n",
    "    land_use_percentage.reset_index(inplace=True)\n",
    "    return land_use_percentage\n",
    "\n",
    "def class_percentage_neighbor(land_use_cell):\n",
    "    # Calculate the percentage of each class for each neighboring grid cell\n",
    "    for i, k in zip([0, 1], [\"100km\", \"10km\"]):\n",
    "        land_use_cell[f'GRID_CELL_NEIGHBOR_{k}'] = (land_use_cell['LATITUDE']).round(i).astype(str) + '_' + (land_use_cell['LONGITUDE']).round(i).astype(str)\n",
    "        # Iterate over the classes to calculate the average percentage of each class for the neighboring grid cells\n",
    "        for j in range(1, 5):\n",
    "            land_use_cell[f'LAND_USE_CLASS_{j}_NEIGHBOR_{k}_AVG'] = land_use_cell.groupby(f'GRID_CELL_NEIGHBOR_{k}')[f'LAND_USE_CLASS_{j}'].transform('mean')\n",
    "    return land_use_cell\n",
    "\n",
    "# Load all csv files in the folder\n",
    "dataframes = []\n",
    "for csv_file in list_unzip_files('input_data/processed'):\n",
    "    if csv_file.endswith('010.csv') and csv_file[:-4] in [x[:-8] for x in list_unzip_files('input_data/raw/land_use')]:\n",
    "        dataframes.append(pd.read_csv(f\"input_data/processed/{csv_file}\"))\n",
    "# Merge the dataframes\n",
    "land_use_data = merge_dataframes(dataframes)\n",
    "# Calculate the percentage of each class for each grid cell and neighbors\n",
    "land_use_cells = class_percentage_cell(land_use_data)\n",
    "land_use_neigh = class_percentage_neighbor(land_use_cells)\n",
    "# Save the dataframes to a csv file\n",
    "land_use_neigh.to_csv('input_data/processed/land_use_010.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ACQ_DATE</th>\n",
       "      <th>ACQ_TIME</th>\n",
       "      <th>OBLAST_ID</th>\n",
       "      <th>GRID_CELL</th>\n",
       "      <th>DAY_OF_YEAR</th>\n",
       "      <th>FIRE_COUNT_CELL</th>\n",
       "      <th>GRID_CELL_NEIGHBOR_100km</th>\n",
       "      <th>FIRE_COUNT_CELL_NEIGHBOR_100km</th>\n",
       "      <th>...</th>\n",
       "      <th>LAND_USE_CLASS_3</th>\n",
       "      <th>LAND_USE_CLASS_4</th>\n",
       "      <th>LAND_USE_CLASS_1_NEIGHBOR_100km_AVG</th>\n",
       "      <th>LAND_USE_CLASS_2_NEIGHBOR_100km_AVG</th>\n",
       "      <th>LAND_USE_CLASS_3_NEIGHBOR_100km_AVG</th>\n",
       "      <th>LAND_USE_CLASS_4_NEIGHBOR_100km_AVG</th>\n",
       "      <th>LAND_USE_CLASS_1_NEIGHBOR_10km_AVG</th>\n",
       "      <th>LAND_USE_CLASS_2_NEIGHBOR_10km_AVG</th>\n",
       "      <th>LAND_USE_CLASS_3_NEIGHBOR_10km_AVG</th>\n",
       "      <th>LAND_USE_CLASS_4_NEIGHBOR_10km_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.09</td>\n",
       "      <td>37.61</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>930</td>\n",
       "      <td>UA14</td>\n",
       "      <td>47.09_37.61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0_38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054961</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.135035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.09</td>\n",
       "      <td>37.61</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1106</td>\n",
       "      <td>UA14</td>\n",
       "      <td>47.09_37.61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0_38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054961</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.135035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.09</td>\n",
       "      <td>37.61</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>930</td>\n",
       "      <td>UA14</td>\n",
       "      <td>47.09_37.61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0_38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054961</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.135035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.15</td>\n",
       "      <td>37.53</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>930</td>\n",
       "      <td>UA14</td>\n",
       "      <td>47.15_37.53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0_38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054961</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.135035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.51</td>\n",
       "      <td>28.74</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>1054</td>\n",
       "      <td>UA18</td>\n",
       "      <td>50.51_28.74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>51.0_29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472694</td>\n",
       "      <td>0.186626</td>\n",
       "      <td>0.295773</td>\n",
       "      <td>0.038138</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LATITUDE  LONGITUDE    ACQ_DATE  ACQ_TIME OBLAST_ID    GRID_CELL  \\\n",
       "0     47.09      37.61  2015-01-01       930      UA14  47.09_37.61   \n",
       "1     47.09      37.61  2015-01-01      1106      UA14  47.09_37.61   \n",
       "2     47.09      37.61  2015-01-01       930      UA14  47.09_37.61   \n",
       "3     47.15      37.53  2015-01-01       930      UA14  47.15_37.53   \n",
       "4     50.51      28.74  2015-01-02      1054      UA18  50.51_28.74   \n",
       "\n",
       "   DAY_OF_YEAR  FIRE_COUNT_CELL GRID_CELL_NEIGHBOR_100km  \\\n",
       "0            1                3                47.0_38.0   \n",
       "1            1                3                47.0_38.0   \n",
       "2            1                3                47.0_38.0   \n",
       "3            1                1                47.0_38.0   \n",
       "4            2                1                51.0_29.0   \n",
       "\n",
       "   FIRE_COUNT_CELL_NEIGHBOR_100km  ... LAND_USE_CLASS_3  LAND_USE_CLASS_4  \\\n",
       "0                               4  ...              1.0               0.0   \n",
       "1                               4  ...              1.0               0.0   \n",
       "2                               4  ...              1.0               0.0   \n",
       "3                               4  ...              1.0               0.0   \n",
       "4                               1  ...              1.0               0.0   \n",
       "\n",
       "   LAND_USE_CLASS_1_NEIGHBOR_100km_AVG  LAND_USE_CLASS_2_NEIGHBOR_100km_AVG  \\\n",
       "0                             0.054961                             0.174428   \n",
       "1                             0.054961                             0.174428   \n",
       "2                             0.054961                             0.174428   \n",
       "3                             0.054961                             0.174428   \n",
       "4                             0.472694                             0.186626   \n",
       "\n",
       "   LAND_USE_CLASS_3_NEIGHBOR_100km_AVG  LAND_USE_CLASS_4_NEIGHBOR_100km_AVG  \\\n",
       "0                             0.619467                             0.135035   \n",
       "1                             0.619467                             0.135035   \n",
       "2                             0.619467                             0.135035   \n",
       "3                             0.619467                             0.135035   \n",
       "4                             0.295773                             0.038138   \n",
       "\n",
       "   LAND_USE_CLASS_1_NEIGHBOR_10km_AVG  LAND_USE_CLASS_2_NEIGHBOR_10km_AVG  \\\n",
       "0                            0.000000                            0.000000   \n",
       "1                            0.000000                            0.000000   \n",
       "2                            0.000000                            0.000000   \n",
       "3                            0.000000                            0.000000   \n",
       "4                            0.111111                            0.095238   \n",
       "\n",
       "   LAND_USE_CLASS_3_NEIGHBOR_10km_AVG  LAND_USE_CLASS_4_NEIGHBOR_10km_AVG  \n",
       "0                            0.000000                            0.000000  \n",
       "1                            0.000000                            0.000000  \n",
       "2                            0.000000                            0.000000  \n",
       "3                            0.000000                            0.000000  \n",
       "4                            0.698413                            0.071429  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the land_use_percentage csv\n",
    "land_use = pd.read_csv('input_data/processed/land_use_010.csv', index_col=0)\n",
    "# Drop the redundant columns\n",
    "land_use.drop(columns=['LATITUDE', 'LONGITUDE', 'CLASS'], inplace=True)\n",
    "for i, k in zip([0, 1], [\"100km\", \"10km\"]):\n",
    "    land_use.drop(columns=[f'GRID_CELL_NEIGHBOR_{k}'], inplace=True)\n",
    "\n",
    "# Merge the land_use_percentage with the fire_sample dataframe\n",
    "aggregated_data = fire_data.merge(land_use, on='GRID_CELL', how='left')\n",
    "\n",
    "# If there are any missing values in the columns for the classes, fill them with the most frequent value\n",
    "for col in land_use.columns:\n",
    "    aggregated_data[col] = aggregated_data[col].fillna(aggregated_data[col].mode()[0])\n",
    "\n",
    "# Save the dataframes to a csv file\n",
    "aggregated_data.to_csv('input_data/processed/aggregated_data_010.csv', index=False)\n",
    "\n",
    "# Display the updated fire_sample dataframe\n",
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATITUDE                                  0.0\n",
      "LONGITUDE                                 0.0\n",
      "ACQ_DATE                                  0.0\n",
      "ACQ_TIME                                  0.0\n",
      "OBLAST_ID                                 0.0\n",
      "GRID_CELL                                 0.0\n",
      "DAY_OF_YEAR                               0.0\n",
      "FIRE_COUNT_CELL                           0.0\n",
      "GRID_CELL_NEIGHBOR_100km                  0.0\n",
      "FIRE_COUNT_CELL_NEIGHBOR_100km            0.0\n",
      "GRID_CELL_NEIGHBOR_10km                   0.0\n",
      "FIRE_COUNT_CELL_NEIGHBOR_10km             0.0\n",
      "FIRE_COUNT_OBLAST                         0.0\n",
      "FIRE_COUNT_CELL_AVG_7D                    0.0\n",
      "FIRE_COUNT_CELL_NEIGHBOR_100km_AVG_7D     0.0\n",
      "FIRE_COUNT_CELL_NEIGHBOR_10km_AVG_7D      0.0\n",
      "FIRE_COUNT_OBLAST_AVG_7D                  0.0\n",
      "FIRE_COUNT_CELL_AVG_30D                   0.0\n",
      "FIRE_COUNT_CELL_NEIGHBOR_100km_AVG_30D    0.0\n",
      "FIRE_COUNT_CELL_NEIGHBOR_10km_AVG_30D     0.0\n",
      "FIRE_COUNT_OBLAST_AVG_30D                 0.0\n",
      "POP_DENSITY_CELL_AVG                      0.0\n",
      "POP_DENSITY_CELL_NEIGHBOR_100km_AVG       0.0\n",
      "POP_DENSITY_CELL_NEIGHBOR_10km_AVG        0.0\n",
      "LAND_USE_CLASS_1                          0.0\n",
      "LAND_USE_CLASS_2                          0.0\n",
      "LAND_USE_CLASS_3                          0.0\n",
      "LAND_USE_CLASS_4                          0.0\n",
      "LAND_USE_CLASS_1_NEIGHBOR_100km_AVG       0.0\n",
      "LAND_USE_CLASS_2_NEIGHBOR_100km_AVG       0.0\n",
      "LAND_USE_CLASS_3_NEIGHBOR_100km_AVG       0.0\n",
      "LAND_USE_CLASS_4_NEIGHBOR_100km_AVG       0.0\n",
      "LAND_USE_CLASS_1_NEIGHBOR_10km_AVG        0.0\n",
      "LAND_USE_CLASS_2_NEIGHBOR_10km_AVG        0.0\n",
      "LAND_USE_CLASS_3_NEIGHBOR_10km_AVG        0.0\n",
      "LAND_USE_CLASS_4_NEIGHBOR_10km_AVG        0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print the number of nan values in each column\n",
    "print(aggregated_data.isnull().sum()/len(aggregated_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Higher resolution land use data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
