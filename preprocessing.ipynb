{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import rasterio as rio\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_unzip_files(folder_path, unzip_dbf=False):\n",
    "    # List all files in the folder\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), folder_path)\n",
    "            all_files.append(relative_path)\n",
    "    # If there are any dbf zip files, unzip them\n",
    "    if unzip_dbf:\n",
    "        for file_path in files:\n",
    "            if file_path.endswith('.zip'):\n",
    "                if not os.path.exists(f\"{folder_path}/{file_path[:-4]}\"):\n",
    "                    with zipfile.ZipFile(f\"{folder_path}/{file_path}\", 'r') as zip_ref:\n",
    "                        zip_ref.extractall(folder_path)\n",
    "    return all_files\n",
    "\n",
    "# Define the folder paths for shapefiles and csv files\n",
    "folder_path = 'input_data/raw/fires'\n",
    "files = list_unzip_files(folder_path, unzip_dbf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files, folder_path):\n",
    "    dataframes = []\n",
    "    # Load all shp and csv files\n",
    "    for file_path in files:\n",
    "        if file_path.endswith('.shp'):\n",
    "            df = gpd.read_file(f\"{folder_path}/{file_path}\")\n",
    "        elif file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(f\"{folder_path}/{file_path}\")\n",
    "        else:\n",
    "            continue\n",
    "        df.columns = map(str.upper, df.columns)\n",
    "        dataframes.append(df)\n",
    "    # Concatenate all dataframes\n",
    "    fire_data = pd.concat(dataframes, ignore_index=True)\n",
    "    return fire_data\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "fire_data = load_data(files, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fires(fire_data):\n",
    "    # Drop duplicates time and location\n",
    "    fire_data.drop_duplicates(subset=['ACQ_DATE', 'ACQ_TIME', 'LATITUDE', 'LONGITUDE'], inplace=True)\n",
    "    # Drop unnecessary columns\n",
    "    fire_data.drop(columns=['COUNTRY_ID', 'SCAN', 'TRACK', 'SATELLITE', 'CONFIDENCE', 'VERSION', 'FRP', 'DAYNIGHT', 'BRIGHTNESS', 'BRIGHT_T31', 'TYPE', 'GEOMETRY', 'INSTRUMENT'], inplace=True)\n",
    "    # Drop rows with missing or nan values\n",
    "    fire_data.dropna(inplace=True)\n",
    "    # Transform the date to datetime format\n",
    "    fire_data['ACQ_DATE'] = fire_data['ACQ_DATE'].apply(lambda x: pd.to_datetime(x).date())\n",
    "    # Transform the time to integer format\n",
    "    fire_data['ACQ_TIME'] = fire_data['ACQ_TIME'].astype(int)\n",
    "    # Reduce the precision of the coordinates using two decimal places, which is approximately 1.1 km\n",
    "    # (https://support.oxts.com/hc/en-us/articles/115002885125-Level-of-Resolution-of-Longitude-and-Latitude-Measurements)\n",
    "    fire_data['LATITUDE'] = fire_data['LATITUDE'].round(2)\n",
    "    fire_data['LONGITUDE'] = fire_data['LONGITUDE'].round(2)\n",
    "    # Add an ID column and make it the first column\n",
    "    fire_data['FIRE_ID'] = fire_data.index\n",
    "    fire_data = fire_data[['FIRE_ID'] + [col for col in fire_data.columns if col != 'FIRE_ID']]\n",
    "    return fire_data\n",
    "\n",
    "# Preprocess the fire data\n",
    "fire_data = preprocess_fires(fire_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fires_date(fire_data, start_date, end_date):\n",
    "    # Filter the fire data based on the input parameters\n",
    "    filtered_data = fire_data[(fire_data['ACQ_DATE'] >= start_date) & (fire_data['ACQ_DATE'] <= end_date)]\n",
    "    # Reset the index\n",
    "    filtered_data.reset_index(drop=True, inplace=True)\n",
    "    return filtered_data\n",
    "\n",
    "# Filter the fire data based on a specific date range\n",
    "start_date = pd.to_datetime('2014-12-01').date() # Need the last month of 2014 for calculating rolling statistics\n",
    "end_date = pd.to_datetime('today').date()\n",
    "fire_data = filter_fires_date(fire_data, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_border(data, path_to_border):\n",
    "    # Load the shapefile containing the administrative borders of Ukraine\n",
    "    ukraine_borders = gpd.read_file(path_to_border)\n",
    "    # Ensure the data is a GeoDataFrame\n",
    "    data_gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data.LONGITUDE, data.LATITUDE))\n",
    "    # Set the same coordinate reference system (CRS) for both GeoDataFrames\n",
    "    data_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    ukraine_borders.set_crs(epsg=4326, inplace=True)\n",
    "    # Perform a spatial join to filter datapoints within Ukraine borders\n",
    "    data_in_ukraine = gpd.sjoin(data_gdf, ukraine_borders, how='inner')\n",
    "    # Drop the geometry column as it's no longer needed\n",
    "    data_in_ukraine.drop(columns=['geometry', 'source', 'name', 'index_right'], inplace=True)\n",
    "    # Reset the index\n",
    "    data_in_ukraine.reset_index(drop=True, inplace=True)\n",
    "    # Make sure all columns are in uppercase\n",
    "    data_in_ukraine.columns = map(str.upper, data_in_ukraine.columns)\n",
    "    # Rename the ID column to OBLAST_ID\n",
    "    data_in_ukraine.rename(columns={'ID': 'OBLAST_ID'}, inplace=True)\n",
    "    # Generate a unique identifier for each grid cell\n",
    "    data_in_ukraine['GRID_CELL'] = data_in_ukraine['LATITUDE'].astype(str) + '_' + data_in_ukraine['LONGITUDE'].astype(str)\n",
    "    return data_in_ukraine\n",
    "\n",
    "# Filter the fire data based on the administrative borders of Ukraine\n",
    "fire_data = filter_data_border(fire_data, 'input_data/raw/ukr_borders/ua.shp') # https://simplemaps.com/gis/country/ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_counts(fire_data):\n",
    "    # sort by date\n",
    "    fire_data = fire_data.sort_values(by='ACQ_DATE')\n",
    "    # Generate the day of the year\n",
    "    fire_data['DAY_OF_YEAR'] = fire_data['ACQ_DATE'].apply(lambda x: x.timetuple().tm_yday)\n",
    "    # Generate the number of fires per grid cell for the specific day (ACQ_DATE)\n",
    "    fire_data['FIRE_COUNT_CELL'] = fire_data.groupby(['GRID_CELL', 'ACQ_DATE'])['ACQ_DATE'].transform('count')\n",
    "    # Generate the number of fires per region (ID) for the specific day (ACQ_DATE)\n",
    "    fire_data['FIRE_COUNT_OBLAST'] = fire_data.groupby(['OBLAST_ID', 'ACQ_DATE'])['ACQ_DATE'].transform('count')\n",
    "    # Assign all rows with a value greater than 500 to 500 to avoid extreme values\n",
    "    fire_data['FIRE_COUNT_OBLAST'] = fire_data['FIRE_COUNT_OBLAST'].apply(lambda x: 500 if x > 500 else x)\n",
    "    # Generate average number of fires per grid cell, neighboring grid cells, and region for the last 7 and 30 days\n",
    "    for i in [7, 30]:\n",
    "        fire_data[f'FIRE_COUNT_CELL_AVG_{i}D'] = fire_data.groupby('GRID_CELL')['FIRE_COUNT_CELL'].transform(lambda x: x.rolling(i, min_periods=1).mean())\n",
    "        fire_data[f'FIRE_COUNT_OBLAST_AVG_{i}D'] = fire_data.groupby('OBLAST_ID')['FIRE_COUNT_OBLAST'].transform(lambda x: x.rolling(i, min_periods=1).mean())\n",
    "        # Assign all rows with a value greater than 500 to 500 to avoid extreme values\n",
    "        fire_data[f'FIRE_COUNT_OBLAST_AVG_{i}D'] = fire_data[f'FIRE_COUNT_OBLAST_AVG_{i}D'].apply(lambda x: 500 if x > 500 else x)\n",
    "    return fire_data\n",
    "\n",
    "# Perform feature engineering\n",
    "fire_data = generate_counts(fire_data)\n",
    "# Filter the fire data based on a specific date range\n",
    "start_date = pd.to_datetime('2015-01-01').date()\n",
    "fire_data = filter_fires_date(fire_data, start_date, end_date)\n",
    "# Save the preprocessed fire data\n",
    "fire_data.to_csv('input_data/processed/fire_data.csv', index=False)\n",
    "# Zip the preprocessed fire data\n",
    "with zipfile.ZipFile('input_data/processed/fire_data.csv.zip', 'w') as zip_ref:\n",
    "    zip_ref.write('input_data/processed/fire_data.csv', 'fire_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_population_density(path_to_population, round_precision=2):\n",
    "    # Load the population density data\n",
    "    pop_density_df = pd.read_csv(path_to_population)\n",
    "    # Rename the columns to be more descriptive\n",
    "    pop_density_df.columns = ['LONGITUDE', 'LATITUDE', 'POP_DENSITY']\n",
    "    # Round the latitude and longitude to match the precision used in fire_data\n",
    "    pop_density_df['LATITUDE'] = pop_density_df['LATITUDE'].round(round_precision)\n",
    "    pop_density_df['LONGITUDE'] = pop_density_df['LONGITUDE'].round(round_precision)\n",
    "    # Calculate the average population density for each grid cell\n",
    "    pop_density_avg = pop_density_df.groupby(['LATITUDE', 'LONGITUDE'])['POP_DENSITY'].mean().reset_index()\n",
    "    # Create a unique identifier for each grid cell\n",
    "    pop_density_avg['GRID_CELL'] = pop_density_avg['LATITUDE'].astype(str) + '_' + pop_density_avg['LONGITUDE'].astype(str)\n",
    "    # Rename the column to indicate it's the population density average per grid cell\n",
    "    pop_density_avg.rename(columns={'POP_DENSITY': 'POP_DENSITY_CELL_AVG'}, inplace=True)\n",
    "    # Assign all rows with a value greater than 250 to 250 to avoid extreme values\n",
    "    pop_density_avg['POP_DENSITY_CELL_AVG'] = pop_density_avg['POP_DENSITY_CELL_AVG'].apply(lambda x: 250 if x > 250 else x)\n",
    "    return pop_density_avg\n",
    "\n",
    "# Calculate the population density for each grid cell\n",
    "pop_density = generate_population_density('input_data/raw/population/ukr_pd_2020_1km_UNadj_ASCII_XYZ.csv')\n",
    "# Save the population density data to a csv file\n",
    "pop_density.to_csv('input_data/processed/pop_density.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fire_pop_data(fire_data, pop_density):\n",
    "    # Drop redundant columns\n",
    "    pop_density.drop(columns=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
    "    # Merge the fire data with the population density data\n",
    "    fire_data = fire_data.merge(pop_density, on='GRID_CELL', how='left')\n",
    "    # Fill missing values with the average population density\n",
    "    fire_data['POP_DENSITY_CELL_AVG'].fillna(fire_data['POP_DENSITY_CELL_AVG'].mean(), inplace=True)\n",
    "    return fire_data\n",
    "\n",
    "# Load the population density data\n",
    "pop_density = pd.read_csv('input_data/processed/pop_density.csv')\n",
    "# Load the preprocessed fire data\n",
    "fire_data = pd.read_csv('input_data/processed/fire_data.csv')\n",
    "# Merge the fire data with the population density data\n",
    "fire_data = merge_fire_pop_data(fire_data, pop_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 36T_20220101-20230101.tif\n"
     ]
    }
   ],
   "source": [
    "# Define the WGS84 projection (EPSG:4326)\n",
    "wgs84_proj = Proj('epsg:4326')\n",
    "\n",
    "def load_tif_with_reduced_resolution(tif_path, scale_factor):\n",
    "    with rio.open(tif_path) as src:\n",
    "        # Calculate the new shape\n",
    "        new_height = int(src.height * scale_factor)\n",
    "        new_width = int(src.width * scale_factor)\n",
    "        # Read the data with resampling\n",
    "        data = src.read(\n",
    "            out_shape=(src.count, new_height, new_width),\n",
    "            resampling=rio.enums.Resampling.bilinear\n",
    "        )\n",
    "        # Scale the transform\n",
    "        transform_scaled = src.transform * src.transform.scale(\n",
    "            (src.width / data.shape[-1]),\n",
    "            (src.height / data.shape[-2])\n",
    "        )\n",
    "        # Get the UTM projection from the metadata\n",
    "        utm_proj = Proj(src.crs)\n",
    "        return data, np.array(transform_scaled), utm_proj\n",
    "\n",
    "def transform_tif_coordinates(data, transform_tif, utm_proj):\n",
    "    # transform the coordinates of the pixels\n",
    "    lon = np.zeros(data.shape[1:])\n",
    "    lat = np.zeros(data.shape[1:])\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(data.shape[2]):\n",
    "            lat[i][j], lon[i][j] = transform(utm_proj, wgs84_proj, transform_tif[2] + transform_tif[0] * j, transform_tif[5] + transform_tif[4] * i)\n",
    "    return lon, lat\n",
    "\n",
    "def convert_to_dataframe(data, lon, lat, map_dict):\n",
    "    # Create a dataframe with the class of the pixel and its longitude and latitude value\n",
    "    df = pd.DataFrame(data[0].flatten(), columns=['class'])\n",
    "    # get longitude and latitude values and round them to 2 decimal places\n",
    "    df['LONGITUDE'] = lon.flatten().round(2)\n",
    "    df['LATITUDE'] = lat.flatten().round(2)\n",
    "    # Delete all rows with class 0 (no data)\n",
    "    df = df[df['class'] != 0]\n",
    "    # Map the classes to the new classes\n",
    "    df['CLASS'] = df['class'].map(map_dict)\n",
    "    df.drop(columns=['class'], inplace=True)\n",
    "    # Make sure all columns are in uppercase\n",
    "    df.columns = map(str.upper, df.columns)\n",
    "    return df\n",
    "\n",
    "# Load the TIFF file with reduced resolution\n",
    "# Reduce resolution by 95%, i.e., 10m resolution to 200m resolution\n",
    "scale_factor = 0.05\n",
    "# Dictionary to merge some of the classes into new classes\n",
    "# Original classes: 0: 'No data', 1: 'Water', 2: 'Trees', 4: 'Flooded vegetation', 5: 'Crops', 7: 'Built Area', 8: 'Bare ground', 9: 'Snow and ice', 10: 'Clouds', 11: 'Rangeland'\n",
    "# New classes: \n",
    "# 0: ['No data', 'Water', 'Snow and ice', 'Clouds'], \n",
    "# 1: ['Trees'], \n",
    "# 2: ['Flooded vegetation', 'Bare ground', 'Rangeland'], \n",
    "# 3: ['Crops'], \n",
    "# 4: ['Built Area']\n",
    "class_transform = {0: 0, 1: 0, 2: 1, 4: 2, 5: 3, 7: 4, 8: 2, 9: 0, 10: 0, 11: 2}\n",
    "\n",
    "# Iterate over all tif files in the folder\n",
    "for tif_file in list_unzip_files('input_data/raw/land_use')[3:]:\n",
    "    if tif_file.endswith('.tif'):\n",
    "        print(f\"Initialized {tif_file}\")\n",
    "        # Load the TIFF file with reduced resolution\n",
    "        data, transform_scaled, utm_proj = load_tif_with_reduced_resolution(f\"input_data/raw/land_use/{tif_file}\", scale_factor)\n",
    "        # Transform the coordinates of the pixels\n",
    "        lon, lat = transform_tif_coordinates(data, transform_scaled, utm_proj)\n",
    "        # Convert the data to a dataframe\n",
    "        land_use_df = convert_to_dataframe(data, lon, lat, class_transform)\n",
    "        # Save dataframe as a csv file\n",
    "        land_use_df.to_csv(f'input_data/processed/{tif_file[:-4]}_005.csv', index=False)\n",
    "        print(f\"Processed {tif_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(dataframes):\n",
    "    # Merge the dataframes\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    # Drop rows that are not in the Ukraine borders\n",
    "    merged_df = filter_data_border(merged_df, 'input_data/raw/ukr_borders/ua.shp')\n",
    "    # Reset the index\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def class_percentage_cell(land_use_data):\n",
    "    # One hot encode the class column in land_use_data\n",
    "    land_use_percentage = pd.get_dummies(land_use_data, columns=['CLASS'], prefix='LAND_USE_CLASS', drop_first=True)\n",
    "    # Drop the OBLAST_ID column as it's not needed\n",
    "    land_use_percentage.drop(columns=['OBLAST_ID'], inplace=True)\n",
    "    # Calculate the percentage of each class for each grid cell\n",
    "    land_use_percentage = land_use_percentage.groupby(['GRID_CELL']).mean()\n",
    "    land_use_percentage.reset_index(inplace=True)\n",
    "    return land_use_percentage\n",
    "\n",
    "# Load all csv files in the folder\n",
    "dataframes = []\n",
    "for csv_file in list_unzip_files('input_data/processed'):\n",
    "    if csv_file.endswith('001.csv') and csv_file[:-8] in [x[:-4] for x in list_unzip_files('input_data/raw/land_use')]:\n",
    "        dataframes.append(pd.read_csv(f\"input_data/processed/{csv_file}\"))\n",
    "# Merge the dataframes\n",
    "land_use_data = merge_dataframes(dataframes)\n",
    "# Calculate the percentage of each class for each grid cell and neighbors\n",
    "land_use_cells = class_percentage_cell(land_use_data)\n",
    "# Save the dataframes to a csv file\n",
    "land_use_cells.to_csv('input_data/processed/land_use_001.csv', index=False)\n",
    "# Zip the land use data\n",
    "with zipfile.ZipFile('input_data/processed/land_use_001.csv.zip', 'w') as zip_ref:\n",
    "    zip_ref.write('input_data/processed/land_use_001.csv', 'land_use_001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values before filling: 0.052973832080296296\n",
      "NaN values after filling: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRE_ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ACQ_DATE</th>\n",
       "      <th>ACQ_TIME</th>\n",
       "      <th>OBLAST_ID</th>\n",
       "      <th>GRID_CELL</th>\n",
       "      <th>DAY_OF_YEAR</th>\n",
       "      <th>FIRE_COUNT_CELL</th>\n",
       "      <th>FIRE_COUNT_OBLAST</th>\n",
       "      <th>FIRE_COUNT_CELL_AVG_7D</th>\n",
       "      <th>FIRE_COUNT_OBLAST_AVG_7D</th>\n",
       "      <th>FIRE_COUNT_CELL_AVG_30D</th>\n",
       "      <th>FIRE_COUNT_OBLAST_AVG_30D</th>\n",
       "      <th>POP_DENSITY_CELL_AVG</th>\n",
       "      <th>LAND_USE_CLASS_1</th>\n",
       "      <th>LAND_USE_CLASS_2</th>\n",
       "      <th>LAND_USE_CLASS_3</th>\n",
       "      <th>LAND_USE_CLASS_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>921182</td>\n",
       "      <td>47.09</td>\n",
       "      <td>37.61</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>930</td>\n",
       "      <td>UA14</td>\n",
       "      <td>47.09_37.61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>7.433333</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>921184</td>\n",
       "      <td>47.09</td>\n",
       "      <td>37.61</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1106</td>\n",
       "      <td>UA14</td>\n",
       "      <td>47.09_37.61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>3.380952</td>\n",
       "      <td>7.266667</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>921183</td>\n",
       "      <td>47.09</td>\n",
       "      <td>37.61</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>930</td>\n",
       "      <td>UA14</td>\n",
       "      <td>47.09_37.61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>921181</td>\n",
       "      <td>47.15</td>\n",
       "      <td>37.53</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>930</td>\n",
       "      <td>UA14</td>\n",
       "      <td>47.15_37.53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.933333</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>921185</td>\n",
       "      <td>50.51</td>\n",
       "      <td>28.74</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>1054</td>\n",
       "      <td>UA18</td>\n",
       "      <td>50.51_28.74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.238095</td>\n",
       "      <td>16.663777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIRE_ID  LATITUDE  LONGITUDE    ACQ_DATE  ACQ_TIME OBLAST_ID    GRID_CELL  \\\n",
       "0   921182     47.09      37.61  2015-01-01       930      UA14  47.09_37.61   \n",
       "1   921184     47.09      37.61  2015-01-01      1106      UA14  47.09_37.61   \n",
       "2   921183     47.09      37.61  2015-01-01       930      UA14  47.09_37.61   \n",
       "3   921181     47.15      37.53  2015-01-01       930      UA14  47.15_37.53   \n",
       "4   921185     50.51      28.74  2015-01-02      1054      UA18  50.51_28.74   \n",
       "\n",
       "   DAY_OF_YEAR  FIRE_COUNT_CELL  FIRE_COUNT_OBLAST  FIRE_COUNT_CELL_AVG_7D  \\\n",
       "0            1                3                  4                1.857143   \n",
       "1            1                3                  4                2.000000   \n",
       "2            1                3                  4                2.142857   \n",
       "3            1                1                  4                1.000000   \n",
       "4            2                1                  1                1.000000   \n",
       "\n",
       "   FIRE_COUNT_OBLAST_AVG_7D  FIRE_COUNT_CELL_AVG_30D  \\\n",
       "0                  8.285714                 3.400000   \n",
       "1                  7.571429                 3.380952   \n",
       "2                  6.857143                 3.363636   \n",
       "3                  6.142857                 1.000000   \n",
       "4                  2.285714                 1.000000   \n",
       "\n",
       "   FIRE_COUNT_OBLAST_AVG_30D  POP_DENSITY_CELL_AVG  LAND_USE_CLASS_1  \\\n",
       "0                   7.433333            250.000000               0.0   \n",
       "1                   7.266667            250.000000               0.0   \n",
       "2                   7.100000            250.000000               0.0   \n",
       "3                   6.933333            250.000000               0.0   \n",
       "4                   4.238095             16.663777               0.0   \n",
       "\n",
       "   LAND_USE_CLASS_2  LAND_USE_CLASS_3  LAND_USE_CLASS_4  \n",
       "0               0.0               1.0               0.0  \n",
       "1               0.0               1.0               0.0  \n",
       "2               0.0               1.0               0.0  \n",
       "3               0.0               1.0               0.0  \n",
       "4               0.0               1.0               0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the land_use_percentage csv\n",
    "land_use = pd.read_csv('input_data/processed/land_use_001.csv', index_col=0)\n",
    "# Drop the redundant columns\n",
    "land_use.drop(columns=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
    "\n",
    "# Merge the land_use_percentage with the fire_sample dataframe\n",
    "aggregated_data = fire_data.merge(land_use, on='GRID_CELL', how='left')\n",
    "\n",
    "# print the number of nan values in each column\n",
    "print(\"NaN values before filling: {}\".format(np.mean(aggregated_data.isnull().sum()/len(aggregated_data))))\n",
    "\n",
    "# If there are any missing values in the columns for the classes, fill them with the most frequent value\n",
    "for col in land_use.columns:\n",
    "    aggregated_data[col] = aggregated_data[col].fillna(aggregated_data[col].mode()[0])\n",
    "\n",
    "# print the number of nan values in each column\n",
    "print(\"NaN values after filling: {}\".format(np.mean(aggregated_data.isnull().sum()/len(aggregated_data))))\n",
    "\n",
    "# Save the dataframes to a csv file\n",
    "aggregated_data.to_csv('input_data/processed/aggregated_data_001.csv', index=False)\n",
    "# Zip the aggregated data\n",
    "with zipfile.ZipFile('input_data/processed/aggregated_data_001.csv.zip', 'w') as zip_ref:\n",
    "    zip_ref.write('input_data/processed/aggregated_data_001.csv', 'aggregated_data_001.csv')\n",
    "\n",
    "# Display the updated fire_sample dataframe\n",
    "aggregated_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
