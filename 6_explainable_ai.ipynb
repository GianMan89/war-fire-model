{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "fire_data = pd.read_csv('input_data/processed/fire_data.csv')\n",
    "cell_static_data = pd.read_csv('input_data/processed/cell_static.csv')\n",
    "cell_dynamic_data = pd.read_csv('input_data/processed/cell_dynamic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep data in the date range from 2022-02-24 to 2024-09-30, i.e., war-time data\n",
    "fire_data['ACQ_DATE'] = pd.to_datetime(fire_data['ACQ_DATE'])\n",
    "cell_dynamic_data['ACQ_DATE'] = pd.to_datetime(cell_dynamic_data['ACQ_DATE'])\n",
    "fire_data = fire_data[(fire_data['ACQ_DATE'] >= '2022-02-24') & (fire_data['ACQ_DATE'] <= '2024-09-30')]\n",
    "cell_dynamic_data = cell_dynamic_data[(cell_dynamic_data['ACQ_DATE'] >= '2022-02-24') & (cell_dynamic_data['ACQ_DATE'] <= '2024-09-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the fire data for later use\n",
    "fire_data_copy = fire_data[['FIRE_ID', 'GRID_CELL_50KM', 'ACQ_DATE', 'LONGITUDE', 'LATITUDE']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "fire_data.drop(columns=['FIRE_ID', 'LATITUDE', 'LONGITUDE', 'GRID_CELL', 'OBLAST_ID', \n",
    "                        'LATITUDE_1KM', 'LONGITUDE_1KM', 'GRID_CELL_1KM', 'OBLAST_ID_1KM', \n",
    "                        'FIRE_COUNT_CELL_1KM',], inplace=True)\n",
    "# Drop duplicates\n",
    "fire_data.drop_duplicates(inplace=True)\n",
    "# Reset index\n",
    "fire_data.reset_index(drop=True, inplace=True)\n",
    "fire_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_static_data(data, static_data, resolution='50KM'):\n",
    "    # Drop all columns in the static data that are not relevant for the specific resolution\n",
    "    static_data = static_data[[col for col in static_data.columns if col.endswith(resolution)]]\n",
    "    # Drop all duplicates\n",
    "    static_data.drop_duplicates(inplace=True)\n",
    "    # Merge the fire data with the static data\n",
    "    merged_data = pd.merge(data, static_data, how='left', on=['GRID_CELL_50KM', 'OBLAST_ID_50KM', 'LATITUDE_50KM', 'LONGITUDE_50KM'])\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the fire data with the static data\n",
    "fire_data_processed = merge_static_data(fire_data, cell_static_data)\n",
    "fire_data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fire_time_series(data, start_date, end_date, resolution='50KM'):\n",
    "    time_series_data = {}\n",
    "    # Iterate over all cells in the grid_cell column\n",
    "    for cell in data['GRID_CELL_{}'.format(resolution)].unique():\n",
    "        # Filter the data for the specific cell\n",
    "        cell_data = data[data['GRID_CELL_{}'.format(resolution)] == cell]\n",
    "        # Save the static data from all columns except the ACQ_DATE, DAY_OF_YEAR, and FIRE_COUNT_CELL columns\n",
    "        static_data = cell_data.iloc[0].drop(['ACQ_DATE', 'DAY_OF_YEAR', 'FIRE_COUNT_CELL_{}'.format(resolution)])\n",
    "        # Set ACQ_DATE as the index and reindex with the complete date range\n",
    "        cell_data.set_index('ACQ_DATE', inplace=True)\n",
    "        cell_data.index = pd.to_datetime(cell_data.index)\n",
    "        cell_data = cell_data.reindex(pd.date_range(start=start_date, end=end_date, freq='D'), fill_value=0)\n",
    "        # Override the DAY_OF_YEAR column with the correct values\n",
    "        cell_data['DAY_OF_YEAR'] = cell_data.index.dayofyear\n",
    "        # Override the ACQ_DATE column with the correct values\n",
    "        cell_data['ACQ_DATE'] = cell_data.index\n",
    "        cell_data.reset_index(drop=True, inplace=True)\n",
    "        cell_data = cell_data[['ACQ_DATE'] + [col for col in cell_data.columns if col != 'ACQ_DATE']]\n",
    "        # Override the columns with the static data\n",
    "        for col in static_data.index:\n",
    "            cell_data[col] = static_data[col]\n",
    "        # Save the data\n",
    "        time_series_data[cell] = cell_data\n",
    "    # Merge the time series data into a single DataFrame\n",
    "    time_series_data = pd.concat(time_series_data.values())\n",
    "    return time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range from 2022-02-24 to 2024-09-30, i.e., war-time data\n",
    "fire_data_processed = generate_fire_time_series(fire_data_processed, '2022-02-24', '2024-09-30', '50KM')\n",
    "fire_data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dynamic_data(data, dynamic_data):\n",
    "    # Merge the fire data with the dynamic data\n",
    "    merged_data = pd.merge(data, dynamic_data, how='left', left_on=['OBLAST_ID_50KM', 'ACQ_DATE'], right_on=['OBLAST_ID', 'ACQ_DATE'])\n",
    "    # Drop the OBLAST_ID column\n",
    "    merged_data.drop(columns=['OBLAST_ID'], inplace=True)\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the fire data with the dynamic data\n",
    "fire_data_processed = merge_dynamic_data(fire_data_processed, cell_dynamic_data)\n",
    "fire_data_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "features = fire_data_processed.drop(columns=['ACQ_DATE', 'GRID_CELL_50KM', 'OBLAST_ID_50KM', 'FIRE_COUNT_CELL_50KM'])\n",
    "target = fire_data_processed['FIRE_COUNT_CELL_50KM']\n",
    "\n",
    "# Get two additional features, i.e., the ACQ_DATE and the GRID_CELL_50KM\n",
    "acq_date = fire_data_processed['ACQ_DATE']\n",
    "grid_cell = fire_data_processed['GRID_CELL_50KM']\n",
    "\n",
    "# Bring the data in the correct format for the sklearn pipeline\n",
    "X_test = features.values\n",
    "y_test = target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def calculate_significance_score(value, threshold):\n",
    "    # Calculate the significance score\n",
    "    if value < threshold:\n",
    "        return ((value - threshold) / threshold)\n",
    "    else:\n",
    "        return (value - threshold) / value\n",
    "\n",
    "class ThresholdStep(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"True values (y) are required for the threshold step.\")\n",
    "        \n",
    "        # Calculate the error\n",
    "        error = y - X\n",
    "        # Set all negative values to zero\n",
    "        error[error < 0] = 0\n",
    "        # Compare the error with the threshold\n",
    "        is_abnormal = error > self.threshold\n",
    "        # Calculate the significance score\n",
    "        significance_score = np.array(pd.Series(error).apply(calculate_significance_score, threshold=self.threshold))\n",
    "\n",
    "        return np.array([is_abnormal.astype(int), significance_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class RecalculateConfidenceScores(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, decay_rate, midpoint, cutoff):\n",
    "        self.decay_rate = decay_rate\n",
    "        self.midpoint = midpoint\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def sigmoid_decay(self, time_diff):\n",
    "        if time_diff > self.cutoff:\n",
    "            return 0  # Influence reaches zero after the cut-off\n",
    "        return 1 / (1 + np.exp(self.decay_rate * (time_diff - self.midpoint)))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, dates=None, grid_cells=None):\n",
    "        if dates is None or grid_cells is None:\n",
    "            raise ValueError(\"Dates and grid cells are required for the recalculation of confidence scores.\")\n",
    "        y_scores = X[1]\n",
    "        y_pred = X[0]\n",
    "        # Combine confidence scores, dates, and grid cells into a single list of events with indices\n",
    "        indexed_events = list(enumerate(zip(y_scores, dates, grid_cells)))\n",
    "        # Sort events by 'date' while preserving their original index\n",
    "        indexed_events_sorted = sorted(indexed_events, key=lambda x: x[1][1])  # Sort by date (the second element of the tuple)\n",
    "        # Initialize recalculated confidence scores with placeholders\n",
    "        recalculated_confidences = [None] * len(y_scores)\n",
    "        # Track the last war-related fire by grid cell\n",
    "        last_war_events = {}\n",
    "        # Loop through each event in the sorted order\n",
    "        for i, (original_index, (current_conf, current_date, grid_cell)) in enumerate(indexed_events_sorted):\n",
    "            # If this is a war-related fire, reset the decay process for this grid cell\n",
    "            if current_conf > 0:\n",
    "                last_war_events[grid_cell] = {\n",
    "                    'ACQ_DATE': current_date,\n",
    "                    'SIGNIFICANCE_SCORE': current_conf\n",
    "                }\n",
    "                recalculated_confidences[original_index] = current_conf  # No decay for the current event\n",
    "            elif grid_cell in last_war_events:\n",
    "                # Calculate the time difference from the last war-related fire in the same grid cell\n",
    "                last_war_event = last_war_events[grid_cell]\n",
    "                time_diff = (current_date - last_war_event['ACQ_DATE'])\n",
    "                # Transform the time difference, which is in nanoseconds, to days\n",
    "                time_diff = time_diff / np.timedelta64(1, 'D')\n",
    "                # Apply the decay function to the subsequent fires in the same grid cell\n",
    "                decayed_influence = self.sigmoid_decay(time_diff) * last_war_event['SIGNIFICANCE_SCORE']\n",
    "                # If the decayed influence is zero or less than the original confidence, keep the original confidence\n",
    "                if decayed_influence > current_conf and decayed_influence > 0:\n",
    "                    new_conf = decayed_influence\n",
    "                else:\n",
    "                    new_conf = current_conf  # Preserve original confidence\n",
    "                \n",
    "                recalculated_confidences[original_index] = new_conf\n",
    "            else:\n",
    "                # If no war-related fire has been detected in this grid cell, keep the original confidence\n",
    "                recalculated_confidences[original_index] = current_conf\n",
    "        \n",
    "        recalculated_confidences = np.array(recalculated_confidences)\n",
    "        labels = np.where(recalculated_confidences > 0, 1, 0)\n",
    "        return [labels, recalculated_confidences, y_pred, y_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline\n",
    "import pickle\n",
    "\n",
    "with open('saved_models/pipeline.pkl', 'rb') as file:\n",
    "    pipeline = pickle.load(file)\n",
    "\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Local Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_test, # TODO - replace with training data\n",
    "    feature_names=features.columns, \n",
    "    class_names=['Normal', 'Abnormal'], \n",
    "    mode='regression'\n",
    ")\n",
    "\n",
    "# Choose an instance to explain\n",
    "instance_idx = 5000\n",
    "instance = X_test[instance_idx]\n",
    "\n",
    "# Get the prediction probabilities for the instance\n",
    "predict_fn = lambda x: pipeline.named_steps['regressor'].predict(\n",
    "    pipeline.named_steps['pca'].transform(pipeline.named_steps['scaler'].transform(x)), quantiles=[0.95]\n",
    "    )\n",
    "\n",
    "# Explain the instance\n",
    "exp = explainer.explain_instance(instance, predict_fn, num_features=X_test.shape[1])\n",
    "\n",
    "# save to temp file\n",
    "exp.save_to_file('output_data/lime_explanation.html')\n",
    "\n",
    "# Show the explanation\n",
    "# exp.show_in_notebook(show_table=True, show_all=True)\n",
    "\n",
    "# Show the explanation as a plot\n",
    "exp.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save explainer to file\n",
    "import pickle\n",
    "\n",
    "with open('output_data/lime_explainer.pkl', 'wb') as file:\n",
    "    pickle.dump(explainer, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
